{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55903a99-516b-44ec-aa07-27107687b884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/config.json...\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/eval_results.txt...\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/merges.txt...\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/model_args.json...\n",
      "- [4 files][449.2 KiB/449.2 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/optimizer.pt...\n",
      "==> NOTE: You are downloading one or more large file(s), which would            \n",
      "run significantly faster if you enabled sliced object downloads. This\n",
      "feature is enabled by default but requires that compiled crcmod be\n",
      "installed (see \"gsutil help crcmod\").\n",
      "\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/pytorch_model.bin...\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/scheduler.pt...\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/special_tokens_map.json...\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/tokenizer_config.json...\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/training_args.bin...\n",
      "Copying gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model/vocab.json...\n",
      "\\ [11 files][  1.6 GiB/  1.6 GiB]    8.7 MiB/s                                  \n",
      "Operation completed over 11 objects/1.6 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://internal.whitehead.ai/daily-dialog/01-24-2022_09_23_11/outputs/best_model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadac65d-e10f-40fb-9be9-6849b96b501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import (\n",
    "    ClassificationModel, ClassificationArgs\n",
    ")\n",
    "\n",
    "model = ClassificationModel(\"deberta\", \"./best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9390e81-df6c-4bbb-960d-0b33e7452841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce567d1fcf44877909cef2d939f8e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9955dc60bc34789b2988e79c708d2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([[\"Say what is the meaning of life?\", \"I dont know\"]])\n",
    "\n",
    "convert_to_label = lambda n: [\"__dummy__ (0), inform (1), question (2), directive (3), commissive (4)\".split(', ')[i] for i in n]\n",
    "convert_to_label(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "727ec026-059a-44d8-94cb-91deb57b7214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset daily_dialog (/home/jupyter/.cache/huggingface/datasets/daily_dialog/default/1.0.0/c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18db82e20f71432a950ebab779b538b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/daily_dialog/default/1.0.0/c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c/cache-d4b9dada34f73591.arrow\n",
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/daily_dialog/default/1.0.0/c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c/cache-7f20f03e2165488c.arrow\n",
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/daily_dialog/default/1.0.0/c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c/cache-bae8ae2fd66903f5.arrow\n"
     ]
    }
   ],
   "source": [
    "# Import daily_dialog dataset\n",
    "from datasets import load_dataset\n",
    "daily_dialog = load_dataset(\"daily_dialog\")\n",
    "\n",
    "# Processor\n",
    "def process_row(batch):    \n",
    "    result = {}\n",
    "    text_a = result[\"text_a\"] = []\n",
    "    text_b = result[\"text_b\"] = []\n",
    "    labels = result[\"labels\"] = []\n",
    "    \n",
    "    acts_batch = batch[\"act\"]\n",
    "    dialog_batch = batch[\"dialog\"]\n",
    "    \n",
    "    for row_idx in range(len(dialog_batch)):\n",
    "        \n",
    "        acts = acts_batch[row_idx]\n",
    "        dialog = dialog_batch[row_idx]\n",
    "        num = len(dialog)\n",
    "        assert num == len(acts)\n",
    "\n",
    "        for idx in range(num):\n",
    "            text_a.append(\"\" if idx == 0 else dialog[idx-1])\n",
    "            text_b.append(dialog[idx])\n",
    "            labels.append(acts[idx])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Process dataset\n",
    "mapped = daily_dialog.map(process_row, batched=True, remove_columns=['dialog', 'act', 'emotion'], load_from_cache_file=True).flatten()\n",
    "\n",
    "# Split model into train, dev etc.\n",
    "train_df = mapped['train'].to_pandas()\n",
    "eval_df = mapped['validation'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc82c71-5173-4aee-8b43-8d7488a432a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c54b8e5f74a44ad880d43ee26a742ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8069 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db23ec24efe4a068821c396aaae98e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1009 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiwank\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/diwank/daily-dialog/runs/3kkgtywa\" target=\"_blank\">glistening-rabbit-83</a></strong> to <a href=\"https://wandb.ai/diwank/daily-dialog\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(\n",
    "    eval_df\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:latest"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
